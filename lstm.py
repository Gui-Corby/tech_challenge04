# -*- coding: utf-8 -*-
"""tech_challenge04.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hXKKllHKeAgQid9_YPrLXjCRocJccl6M
"""

import yfinance as yf

symbol = "TSLA" # Tesla, Inc
start_date = "2023-01-01"
end_date = "2024-12-31"

df = yf.download(symbol, start=start_date, end=end_date)

df.columns

df.columns = ['_'.join(col).strip() for col in df.columns.values]
df.rename(columns={'Close_TSLA': 'Close', 'High_TSLA': 'High', 'Low_TSLA': 'Low', 'Open_TSLA': 'Open', 'Volume_TSLA': 'Volume'}, inplace=True)

print("Flattened and renamed columns:")
display(df.columns)
display(df.head())

df.head()

"""# Creating new features "Moving average" and "RSI"
"""

import pandas_ta as ta

# 10-period Simple Moving Average (SMA)
df['SMA'] = ta.sma(df['Close'], length=10)

# 14-period Relative Strength Index (RSI)
df['RSI'] = ta.rsi(df['Close'], length=14)

df.tail()

df.head()



print(df.dtypes)

import numpy as np

cols_price = ['Close', 'High', 'Low', 'Open', "SMA"]
col_volume = "Volume"
col_rsi = "RSI"

# Applying logarithm to volume values
df["Volume_log"] = np.log1p(df[col_volume])

df['RSI_scaled'] = df[col_rsi] / 100.0

# Features entering the model (after transformation)
feature_cols = cols_price + ["Volume_log", "RSI_scaled"]

df[feature_cols].isna().sum() # RSI_scaled contains a null row
df = df.dropna(subset=feature_cols + ["Close"]).reset_index(drop=True)

"""# Splitting between train and test"""

train_size = int(len(df) * 0.8)
train_df = df.iloc[:train_size].copy()
test_df = df.iloc[train_size:].copy()

"""# Normalization"""

from sklearn.preprocessing import MinMaxScaler

scaler_price = MinMaxScaler(feature_range=(0, 1))
scaler_volume = MinMaxScaler(feature_range=(0, 1))

train_df[cols_price] = scaler_price.fit_transform(train_df[cols_price])
test_df[cols_price] = scaler_price.transform(test_df[cols_price])

# We only apply .fit_transform to the train set
train_df[["Volume_log"]] = scaler_volume.fit_transform(train_df[["Volume_log"]])

# We only apply .transform to the test set
test_df[["Volume_log"]] = scaler_volume.transform(test_df[["Volume_log"]])

# Transforming in numpy arrays

X_train_all = train_df[feature_cols].values
X_test_all = test_df[feature_cols].values

close_index = cols_price.index("Close")

y_train_all = train_df[cols_price].values[:, close_index]
y_test_all = test_df[cols_price].values[:, close_index]

"""# Creating sequences"""

def create_sequences(X, y, seq_length):
  X_seqs, y_seqs = [], []
  for i in range(len(X) - seq_length):
    X_seqs.append(X[i : i + seq_length])
    y_seqs.append(y[i + seq_length])
  return np.array(X_seqs), np.array(y_seqs)

seq_length = 20

X_train_seq, y_train = create_sequences(X_train_all, y_train_all, seq_length)
X_test_seq, y_test = create_sequences(X_test_all, y_test_all, seq_length)

"""# Converting to tensors"""

import torch

X_train_tensor = torch.tensor(X_train_seq, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.float32)

X_test_tensor = torch.tensor(X_test_seq, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test, dtype=torch.float32)

y_train_tensor = y_train_tensor.view(-1, 1)
y_test_tensor = y_test_tensor.view(-1, 1)

print("NaN em X_train_tensor?", torch.isnan(X_train_tensor).any())
print("NaN em y_train_tensor?", torch.isnan(y_train_tensor).any())

"""# DataLoader"""

from torch.utils.data import TensorDataset, DataLoader

batch_size = 32

train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

"""# Preparing the model"""

import torch
import torch.nn as nn

class LSTMRegressor(nn.Module):
  def __init__(self, input_size, hidden_size, num_layers, dropout):
    super().__init__()
    self.lstm = nn.LSTM(
    input_size=input_size,
    hidden_size = hidden_size,
    num_layers = num_layers,
    batch_first=True, # -> input: (batch, seq, feature)
    dropout = dropout if num_layers > 1 else 0.0
    )

    # Layer to map from hidden -> 1 value (regression)
    self.fc = nn.Linear(hidden_size, 1)

  def forward(self, x):
    """
    x: (batch_size, leq_length, input_size)
    """

    # out: (batch_size, seq_length, hidden_size)
    # h_n: (num_layers, batch_size, hidden_size)
    out, (h_n, c_n) = self.lstm(x)

    # We only get the output from the last timestep
    # out[:, 1, :] => (batch_size, hidden_size)
    last_hidden = out[:, -1, :]

    out = self.fc(last_hidden)

    return out

"""# Instantiating the model"""

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
print("Device:", device)

input_size = len(feature_cols)
print(input_size)

model = LSTMRegressor(input_size=input_size, hidden_size=64, num_layers=2, dropout=0.0)
model = model.to(device)

criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

num_epochs = 20

for epoch in range(num_epochs):
  model.train()
  epoch_loss = 0.0

  for X_batch, y_batch in train_loader:
    X_batch = X_batch.to(device)
    y_batch = y_batch.to(device)

    optimizer.zero_grad()

    y_pred = model(X_batch)
    loss = criterion(y_pred, y_batch)

    loss.backward()
    optimizer.step()

    epoch_loss += loss.item() * X_batch.size(0)

  epoch_loss /= len(train_loader.dataset)
  print(f"Epoch {epoch+1}/{num_epochs} - train MSE: {epoch_loss:.6f}")

"""# Evaluating the model"""

model.eval()
test_loss = 0.0

with torch.no_grad():
  for X_batch, y_batch in test_loader:
    X_batch = X_batch.to(device)
    y_batch = y_batch.to(device)

    y_pred = model(X_batch)
    loss = criterion(y_pred, y_batch)

    test_loss += loss.item() * X_batch.size(0)

test_loss /= len(test_loader.dataset)
print(f"Test MSE: {test_loss:.6f}")

model.eval()

all_preds_scaled = []
all_targets_scaled = []

with torch.no_grad():
  for X_batch, y_batch in test_loader:
    X_batch = X_batch.to(device)
    y_batch = y_batch.to(device)

    y_pred = model(X_batch)

    all_preds_scaled.append(y_pred.cpu().numpy())
    all_targets_scaled.append(y_batch.cpu().numpy())

y_pred_scaled = np.concatenate(all_preds_scaled, axis=0)
y_true_scaled = np.concatenate(all_targets_scaled, axis=0)

print("Shapes (scaled):", y_pred_scaled.shape, y_true_scaled.shape)

"""# Transforming the prediction back to USD"""

n_test = y_pred_scaled.shape[0]

# Temporary array with len(cols_price)
temp_pred = np.zeros((n_test, len(cols_price)))
temp_true = np.zeros((n_test, len(cols_price)))

# Putting Close (scaled) in the correct column
temp_pred[:, close_index] = y_pred_scaled[:, 0]
temp_true[:, close_index] = y_true_scaled[:, 0]

# Inverting the scale
temp_pred_inv = scaler_price.inverse_transform(temp_pred)
temp_true_inv = scaler_price.inverse_transform(temp_true)

# Getting "Close" column in the original scale
y_pred_real = temp_pred_inv[:, close_index]
y_true_real = temp_true_inv[:, close_index]

print("Shapes (real):", y_pred_real.shape, y_true_real.shape)
print("5 first predicted values:", y_pred_real[:5])
print("5 first real values:", y_true_real[:5])